{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinitSR7/How-to-use-HuggingFace-Transformer-BERT-/blob/master/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0FuSWT-MQrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc5ACIDtw4qU",
        "colab_type": "text"
      },
      "source": [
        "## How to Use Models as Word Embedding Models?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAw9r0mYMVzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
        "          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n",
        "          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n",
        "          (CTRLModel,       CTRLTokenizer,       'ctrl'),\n",
        "          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
        "          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\n",
        "          (RobertaModel,    RobertaTokenizer,    'roberta-base'),\n",
        "          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "         ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bC2MTJfUF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can use any one from above or https://huggingface.co/transformers/pretrained_models.html\n",
        "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
        "model = TransfoXLModel.from_pretrained('transfo-xl-wt103')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcD6WvfBUwic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode text\n",
        "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "with torch.no_grad():\n",
        "  last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3TvV2sCVZnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10056435-ba95-42be-e961-3dcdab79c780"
      },
      "source": [
        "last_hidden_states.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 1024])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfuj_mhjxU2a",
        "colab_type": "text"
      },
      "source": [
        "## Different Types of Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5dH1eYZSox2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
        "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
        "                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-L_YyjpTxlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEFJ6dMVTxeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class = BertForNextSentencePrediction\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "# Models can return full list of hidden-states & attentions weights at each layer\n",
        "model = model_class.from_pretrained(pretrained_weights,\n",
        "                                        output_hidden_states=True,\n",
        "                                        output_attentions=True)\n",
        "input_ids = torch.tensor([tokenizer.encode(\"I am very hungry!\")])\n",
        "all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
        "\n",
        "# Models are compatible with Torchscript\n",
        "model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n",
        "traced_model = torch.jit.trace(model, (input_ids,))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq6U8145Xk6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1cc4461-87dd-48b0-d9cf-6b77f2faec0d"
      },
      "source": [
        "len(all_hidden_states)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26oQ0geSvw_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djpjYYNzvDw4",
        "colab_type": "text"
      },
      "source": [
        "### Training The Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoGBtHZqwf8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please install tensorflow : pip install --upgrade tensorflow==2.0.0-rc1\n",
        "# Please install transformers 2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHQnFrvIjdHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets\n",
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-j_VGIzjwZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7b103a44-9fd9-4b5f-b934-c40ed3d11e98"
      },
      "source": [
        "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
        "data = tensorflow_datasets.load('glue/mrpc')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset glue (/root/tensorflow_datasets/glue/mrpc/0.0.2)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/glue/mrpc/0.0.2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcyUV3aCjxrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
        "# if MrpcProcessor problem arise refer : https://stackoverflow.com/questions/59465885/huggingface-transformers-attributeerror-mrpcprocessor-object-has-no-attribu\n",
        "\n",
        "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
        "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
        "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
        "valid_dataset = valid_dataset.batch(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ey2AD2wjyGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIEvxeOWjx69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a944d618-939c-4318-9e1a-d8fecc3320e1"
      },
      "source": [
        "# Train and evaluate using tf.keras.Model.fit()\n",
        "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
        "                    validation_data=valid_dataset, validation_steps=7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 115 steps, validate for 7 steps\n",
            "Epoch 1/2\n",
            " 15/115 [==>...........................] - ETA: 1:10:07 - loss: 0.6483 - accuracy: 0.6375"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgVZjKWGwspI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
        "sentence_0 = \"This research was consistent with his findings.\"\n",
        "sentence_1 = \"His findings were compatible with this research.\"\n",
        "sentence_2 = \"His findings were not compatible with this research.\"\n",
        "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
        "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fvjf67bwyGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
        "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqs6iU7Rw0wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
        "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joXqglnBw2iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}